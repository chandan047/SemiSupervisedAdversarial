{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import os\n",
    "# from datetime import datetime\n",
    "from os import path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "import torch\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import WEIGHTS_NAME, CONFIG_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "\n",
    "from dataset import *\n",
    "from model import SequenceClassification, MODEL_CLASSES\n",
    "from utils import (evaluate, evaluate_metrics, \n",
    "                   save_metrics, load_metrics, predict_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dateTimeObj = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=64, early_stopping_thresh=0.1, ens_iter=2, eval_bs=600, label='Label', learning_rate=2e-05, model_name='bert', model_path='models_testing', n_folds=5, num_epochs=2, thresh=0.9)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--num_epochs', type=int, default=2)\n",
    "parser.add_argument('--learning_rate', type=float, default=2e-5)\n",
    "parser.add_argument('--batch_size', type=int, default=64)\n",
    "parser.add_argument('--ens_iter', type=int, default=2)\n",
    "parser.add_argument('--thresh', type=float, default=0.9)\n",
    "parser.add_argument('--n_folds', type=int, default=5)\n",
    "parser.add_argument('--label', type=str, default='Label')\n",
    "parser.add_argument('--early_stopping_thresh', type=float, default=0.1)\n",
    "parser.add_argument('--model_path', type=str, default='models_testing')\n",
    "parser.add_argument('--model_name', type=str, default='bert')\n",
    "parser.add_argument('--eval_bs', type=int, default=600)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(args=[])\n",
    "print (args)\n",
    "\n",
    "if not path.isdir(args.model_path):\n",
    "    os.mkdir(args.model_path)\n",
    "\n",
    "import json\n",
    "with open(args.model_path + '/args.txt', 'w') as f:\n",
    "    json.dump(args.__dict__, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          optimizer,\n",
    "          train_loader,\n",
    "          valid_loader,\n",
    "          iteration,\n",
    "          model_path,\n",
    "          num_epochs = 1,\n",
    "          early_stopping_thresh=None):\n",
    "    \n",
    "    best_valid_loss = float(\"Inf\")\n",
    "    eval_every = len(train_loader) // 6\n",
    "    \n",
    "    weights_path = path.join(model_path, WEIGHTS_NAME)\n",
    "    config_path = path.join(model_path, CONFIG_NAME)\n",
    "    metrics_path = path.join(model_path, 'metrics.pt')\n",
    "    plot_path = path.join(model_path, 'plot_losses.png')\n",
    "\n",
    "    average_train_loss = 0.0\n",
    "    average_valid_loss = 0.0\n",
    "\n",
    "    # initialize running values\n",
    "    running_loss = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    global_step = 0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    global_steps_list = []\n",
    "    \n",
    "    # training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            labels = batch[4].type(torch.LongTensor) \n",
    "            labels = labels.to(device) \n",
    "            b_masks = batch[3].type(torch.LongTensor) \n",
    "            b_masks = b_masks.to(device) \n",
    "            masks = batch[1].type(torch.LongTensor) \n",
    "            masks = masks.to(device) \n",
    "            b_comments = batch[2].type(torch.LongTensor)  \n",
    "            b_comments = b_comments.to(device)\n",
    "            comments = batch[0].type(torch.LongTensor)  \n",
    "            comments = comments.to(device)\n",
    "            outputs = model(input_ids=comments, input_ids_adv=b_comments,\n",
    "                            attention_mask=masks, attention_mask_adv=b_masks,\n",
    "                            labels=labels)\n",
    "            loss, logits = outputs[:2]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update running values\n",
    "            running_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # evaluation step\n",
    "            if global_step % eval_every == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():                    \n",
    "\n",
    "                    # validation loop\n",
    "                    for batch in valid_loader:\n",
    "                        labels = batch[4].type(torch.LongTensor)           \n",
    "                        labels = labels.to(device)\n",
    "                        b_masks = batch[3].type(torch.LongTensor) \n",
    "                        b_masks = b_masks.to(device) \n",
    "                        masks = batch[1].type(torch.LongTensor) \n",
    "                        masks = masks.to(device) \n",
    "                        b_comments = batch[2].type(torch.LongTensor)  \n",
    "                        b_comments = b_comments.to(device)\n",
    "                        comments = batch[0].type(torch.LongTensor)  \n",
    "                        comments = comments.to(device)\n",
    "                        outputs = model(input_ids=comments, input_ids_adv=b_comments,\n",
    "                                        attention_mask=masks, attention_mask_adv=b_masks,\n",
    "                                        labels=labels)\n",
    "                        loss, logits = outputs[:2]\n",
    "                        \n",
    "                        valid_running_loss += loss.item()\n",
    "\n",
    "                # evaluation\n",
    "                average_train_loss = running_loss / eval_every\n",
    "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
    "                train_loss_list.append(average_train_loss)\n",
    "                valid_loss_list.append(average_valid_loss)\n",
    "                global_steps_list.append(global_step)\n",
    "\n",
    "                # resetting running values\n",
    "                running_loss = 0.0                \n",
    "                valid_running_loss = 0.0\n",
    "                model.train()\n",
    "\n",
    "                # print progress\n",
    "                print('\\nEpoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
    "                              average_train_loss, average_valid_loss))\n",
    "                \n",
    "                # checkpoint\n",
    "                if best_valid_loss > average_valid_loss:\n",
    "                    best_valid_loss = average_valid_loss\n",
    "                    # model.save_pretrained(model_path)\n",
    "                    model.config.save_pretrained(model_path)\n",
    "                    torch.save(model.state_dict(), weights_path)\n",
    "                    save_metrics(metrics_path, train_loss_list, valid_loss_list, global_steps_list)\n",
    "\n",
    "                if early_stopping_thresh is not None:\n",
    "                    if average_valid_loss - average_train_loss > args.early_stopping_thresh:\n",
    "                        break\n",
    "\n",
    "        if early_stopping_thresh is not None:\n",
    "            if average_valid_loss - average_train_loss > args.early_stopping_thresh:\n",
    "                print (\"Early stopping\")\n",
    "                break\n",
    "    \n",
    "    save_metrics(metrics_path, train_loss_list, valid_loss_list, global_steps_list)\n",
    "    \n",
    "    train_loss_list, valid_loss_list, global_steps_list = load_metrics(metrics_path)\n",
    "    plt.plot(global_steps_list, train_loss_list, label='Train')\n",
    "    plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
    "    plt.xlabel('Global Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(plot_path)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(model_name, init):\n",
    "    _, _, tokenizer_class, options_name = MODEL_CLASSES[model_name]\n",
    "    \n",
    "    if init:\n",
    "        model_path = options_name\n",
    "    else:\n",
    "        model_path = path.join(args.model_path, model_name)\n",
    "    \n",
    "    print (\"Loading tokenizer from {}\".format(model_path))\n",
    "    return tokenizer_class.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "def save_tokenizer(tokenizer, model_name):\n",
    "    model_path = path.join(args.model_path, model_name)\n",
    "    tokenizer.save_pretrained(model_path)\n",
    "    print (\"Saving Tokenizer in {}\".format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model albert\n",
      "Loading tokenizer from models_testing/albert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenize: 8411it [00:04, 1965.75it/s]\n",
      "b_Tokenize: 8411it [00:04, 1899.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed unlabeled samples from models_testing/albert/unlabeled_processed_0.pt\n",
      "Model run 0 fold 0 already trained\n",
      "Test set evaluation\n",
      "Get iterator of dataset with length 8411\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7140    0.6737    0.6933      4999\n",
      "           1     0.5833    0.2083    0.3070      1613\n",
      "           2     0.2396    0.6853    0.3551       464\n",
      "           3     0.5695    0.8795    0.6913       531\n",
      "           4     0.0732    0.1250    0.0923        24\n",
      "           5     0.5796    0.6910    0.6304       780\n",
      "\n",
      "    accuracy                         0.5981      8411\n",
      "   macro avg     0.4599    0.5438    0.4616      8411\n",
      "weighted avg     0.6394    0.5981    0.5929      8411\n",
      "\n",
      "Unlabeled set pseudo-labelling\n",
      "Model:albert loading labels for unlabeled data\n",
      "\n",
      "\n",
      "\n",
      "Model bert\n",
      "Loading tokenizer from models_testing/bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenize: 8411it [00:05, 1435.14it/s]\n",
      "b_Tokenize: 8411it [00:05, 1407.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed unlabeled samples from models_testing/bert/unlabeled_processed_0.pt\n",
      "Model run 0 fold 0 already trained\n",
      "Test set evaluation\n",
      "Get iterator of dataset with length 8411\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7095    0.6233    0.6636      4999\n",
      "           1     0.4644    0.2306    0.3082      1613\n",
      "           2     0.2559    0.6724    0.3708       464\n",
      "           3     0.5918    0.9284    0.7229       531\n",
      "           4     0.5000    0.2500    0.3333        24\n",
      "           5     0.4922    0.7282    0.5874       780\n",
      "\n",
      "    accuracy                         0.5786      8411\n",
      "   macro avg     0.5023    0.5722    0.4977      8411\n",
      "weighted avg     0.6093    0.5786    0.5750      8411\n",
      "\n",
      "Unlabeled set pseudo-labelling\n",
      "Model:bert loading labels for unlabeled data\n",
      "\n",
      "\n",
      "\n",
      "Model distilBert\n",
      "Loading tokenizer from models_testing/distilBert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenize: 8411it [00:06, 1387.67it/s]\n",
      "b_Tokenize: 8411it [00:06, 1387.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed unlabeled samples from models_testing/distilBert/unlabeled_processed_0.pt\n",
      "Model run 0 fold 0 already trained\n",
      "Test set evaluation\n",
      "Get iterator of dataset with length 8411\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7325    0.6339    0.6797      4999\n",
      "           1     0.5289    0.3577    0.4268      1613\n",
      "           2     0.2409    0.6961    0.3579       464\n",
      "           3     0.6239    0.8343    0.7139       531\n",
      "           4     0.3125    0.2083    0.2500        24\n",
      "           5     0.5933    0.7051    0.6444       780\n",
      "\n",
      "    accuracy                         0.6024      8411\n",
      "   macro avg     0.5053    0.5726    0.5121      8411\n",
      "weighted avg     0.6454    0.6024    0.6111      8411\n",
      "\n",
      "Unlabeled set pseudo-labelling\n",
      "Model:distilBert loading labels for unlabeled data\n",
      "\n",
      "\n",
      "\n",
      "Agreement in ensemble is 0.00\n",
      "Ensemble iteration 0\n",
      "Total samples 3000\n",
      "Original test file: /dgxhome/cra5302/MMHS/Train/unlabeled.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b54be3f0e7e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_ensemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     unlabeled_df = add_ensemble_data(y_pred, y_conf, LABEL=args.label, \n\u001b[0;32m---> 82\u001b[0;31m                                      ensemble_home=args.model_path, iteration=iteration)\n\u001b[0m",
      "\u001b[0;32m~/MMHS/MMHS_dataset.py\u001b[0m in \u001b[0;36madd_ensemble_data\u001b[0;34m(labels, confs, LABEL, ensemble_home, iteration)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Ensembled test file:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensemble_home\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mNEWUNLABELED\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0munlabeled_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLABEL\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch36/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3486\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3487\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch36/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3563\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3564\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3565\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch36/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3748\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3749\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3750\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3751\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch36/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Length of values does not match length of index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "for iteration in range(0, 2):\n",
    "    outputs = []\n",
    "    \n",
    "    for model_name in MODEL_CLASSES:\n",
    "        print (\"Model {}\".format(model_name))\n",
    "        \n",
    "        # paths\n",
    "        model_path = path.join(args.model_path, model_name)\n",
    "        if not path.exists(model_path): \n",
    "            os.mkdir(model_path)\n",
    "        \n",
    "        iteration_path = path.join(model_path, 'run' + str(iteration))\n",
    "        \n",
    "        # tokenizer and dataset\n",
    "        try:\n",
    "            tokenizer = get_tokenizer(model_name, False)\n",
    "        except:\n",
    "            tokenizer = get_tokenizer(model_name, True)\n",
    "        \n",
    "        trainval_iter, test_data, unlabeled_data = get_dataset(ensemble_home=args.model_path, \n",
    "                                                               cache_path=model_path,\n",
    "                                                               iteration=iteration, label=args.label, \n",
    "                                                               tokenizer=tokenizer) \n",
    "        \n",
    "        for fold in range(1):\n",
    "            curr_path = iteration_path + \"_\" + str(fold)\n",
    "        \n",
    "            if path.exists(path.join(curr_path, WEIGHTS_NAME)): \n",
    "                print (\"Model run {} fold {} already trained\".format(iteration, fold)) \n",
    "                model = SequenceClassification(model_name=model_name, model_path=curr_path).to(device) \n",
    "                model.load_state_dict(torch.load(path.join(curr_path, WEIGHTS_NAME), map_location=device))\n",
    "            else:\n",
    "                print (\"Training model {} run {} fold {}\".format(model_name, iteration, fold)) \n",
    "\n",
    "                if not path.exists(curr_path):\n",
    "                    os.mkdir(curr_path) \n",
    "                \n",
    "                save_tokenizer(tokenizer, model_name) \n",
    "\n",
    "                # data and iterator\n",
    "                train_data, weights, val_data = next(trainval_iter)\n",
    "                train_iter = get_iterator(dataset=train_data, batch_size=args.batch_size, shuffle=True, weights = weights)\n",
    "                valid_iter = get_iterator(dataset=val_data, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "                # model and train\n",
    "                model = SequenceClassification(model_name=model_name).to(device)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "                train(model=model, optimizer=optimizer, \n",
    "                        model_path=curr_path, \n",
    "                        early_stopping_thresh=None,\n",
    "                        iteration=iteration, num_epochs=args.num_epochs,\n",
    "                        train_loader=train_iter, valid_loader=valid_iter)\n",
    "\n",
    "                # evaluation - validation\n",
    "                print (\"Validation set evaluation\")\n",
    "                valid_iter = get_iterator(dataset=val_data, batch_size=args.eval_bs, shuffle=False)\n",
    "                evaluate_metrics(model, valid_iter)\n",
    "\n",
    "            # evaluation - test\n",
    "            print (\"Test set evaluation\")\n",
    "            test_iter = get_iterator(dataset=test_data, batch_size=args.eval_bs, shuffle=False) \n",
    "            evaluate_metrics(model, test_iter) \n",
    "\n",
    "            # evaluate - unlabeled\n",
    "            print (\"Unlabeled set pseudo-labelling\")\n",
    "            output_path = path.join(curr_path, \"output.pt\")\n",
    "            if path.exists(output_path):\n",
    "                print (\"Model:{} loading labels for unlabeled data\".format(model_name))\n",
    "                output = torch.load(output_path)\n",
    "            else:\n",
    "                unlabeled_iter = get_iterator(dataset=unlabeled_data, batch_size=args.eval_bs, shuffle=False) \n",
    "                output = evaluate(model, unlabeled_iter)\n",
    "                print (\"Model:{} saving labels for unlabeled data\".format(model_name))\n",
    "                torch.save(output, output_path)\n",
    "\n",
    "            outputs.append(output)\n",
    "\n",
    "            print (\"\\n\\n\")\n",
    "    \n",
    "    y_pred, y_conf = predict_ensemble(outputs)\n",
    "    unlabeled_df = add_ensemble_data(y_pred, y_conf, LABEL=args.label, \n",
    "                                     ensemble_home=args.model_path, iteration=iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch36)",
   "language": "python",
   "name": "torch36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
